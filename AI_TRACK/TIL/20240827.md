### 전문반 TIL
## LLM은 아니지만 예전에했던 회귀 분석 과제를 올려봅니다(날먹아님) ISLP 책을 읽고 내부 문제를 풀었던 내용입니다.

# 1. For each of parts (a) through (d), indicate whether we would generall expect the performance of a fexible statistical learning method to be better or worse than an infexible method. Justify your answer.
## (a) The sample size n is extremely large, and the number of predictors p is small.
A flexible method is expected to be better.
표본 크기가 매우 크고 예측 변수의 수가 작기 때문에 더 fexible한 방법을 사용하면 매우 큰 표본 크기로 인해 noise을 맞추지 않으면서 데이터를 더 잘 맞출 수 있다. 유연한 모델은 과적합 위험이 크지 않고 편향이 적다는 장점이 있다.
## (b) Awful lot of predictors, small sample
A flexible method is expected to be worse.
예측 변수의 수가 매우 많고 관찰의 수가 작은 경우 유연한 모델이 noise를 피팅할 가능성이 매우 높다. 동일한 분포의 다른 무작위 데이터 세트가 주어지면 피팅이 크게 다를 수 있다. 따라서 덜 유연한 방법을 사용하는 것이 더 나을 것입니다. 이 방법은 편향이 더 크지만 과적합 가능성은 낮다.
## (c) Highly non-linear relationship
A flexible method is expected to be better.
그림 2-11 참고
매우 비선형적인 관계를 모델링하려면 보다 유연한 방법이 필요할 수 있다. 그렇지 않으면 모델이 너무 편향되어 모델의 비선형성을 포착하지 못할 것입니다. 표본 크기가 아무리 크더라도 유연성이 떨어지는 모델은 항상 제한된다
## (d) Extremely high variance
A flexible method is expected to be worse.
분산은 다른 훈련세트로f를 추정했을 때 f^의 변동, 유연성이낮을수록분산은 작아짐
분산이 매우 높기 때문에 더 유연한 모델은 noise에 더 잘 맞고 과적합 가능성이 높다. 

# 3. We now revisit the bias-variance decomposition.
## (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less fexible statistical learning methods towards more fexible approaches. The x-axis should represent the amount of fexibility in the method, and the y-axis should represent the values for each curve. There should be fve curves. Make sure to label each one.
```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0.0, 10.0, 0.02)

def squared_bias(x):
    return .002*(-x+10)**3
def variance(x):
    return .002*x**3 
def training_error(x):
    return 2.38936 - 0.825077*x + 0.176655*x**2 - 0.0182319*x**3 + 0.00067091*x**4
def test_error(x):
    return 3 - 0.6*x + .06*x**2
def bayes_error(x):
    return x + 1 - x

plt.rcdefaults()
plt.figure(figsize=(10, 8))
plt.plot(x,squared_bias(x), label='squared bias')
plt.plot(x, variance(x), label='variance')
plt.plot(x, training_error(x), label='training error')
plt.plot(x, test_error(x), label='test error')
plt.plot(x, bayes_error(x), label='Bayes error')
plt.legend(loc='upper center')
plt.xlabel('model flexibility')
plt.show()
```
![image](https://github.com/user-attachments/assets/1f72d8a7-e259-44b6-92c0-aa3cf971f09d)
## (b) Explain why each of the fve curves has the shape displayed inpart (a).
Squared bias.
이는 근사치와 실제 기본 함수의 차이로 인해 발생하는 모델의 오류이다. 보다 유연한 모델은 점점 유사해지며, 따라서 유연성이 증가함에 따라 제곱 편향이 감소한다.

Variance.
유연성이 없는 모델의 한계에서는 모델 적합성이 데이터와 독립적이므로 분산이 0이 된다. 유연성이 증가하면 특정 훈련 세트의 노이즈가 이에 따라 모델에 의해 캡처되므로 분산도 증가한다. 분산으로 설명되는 곡선은 모델 유연성의 단조 증가 함수이다.
제곱편향과 분산은 서로 교환 작용이 일어난다. 제곱편향이 작아질수록 분산은 커지고, 제곱편향이 커질수록 분산은 작아진다.

Training error.
훈련 오류는 모델 예측과 관측치 간의 평균(제곱) 차이로 제공된다. 모델의 유연성이 매우 낮으면 이 수치는 상당히 높을 수 있지만 유연성이 증가하면 이 차이는 줄어든다. 예를 들어 다항식을 고려하면 모델의 유연성을 높이는 것은 적합할 다항식의 정도를 높이는 것을 의미할 수 있다. 추가 자유도는 평균 차이를 줄이고 훈련 오류를 줄인다.

Test error.
예상되는 테스트 오류는 분산 + 편향 + 베이즈 오류 공식으로 제공되며 모두 음수가 아니다. 베이즈 오류는 일정하며 테스트 오류의 하한이다. 테스트 오류는 중간 수준의 유연성에서 최소값을 갖는다. 너무 유연하지 않아 분산이 지배적이지 않고 너무 유연하지 않아 제곱 편향이 너무 높지 않다. 따라서 테스트 오류의 플롯은 일종의 상향(변형된) 포물선과 유사하다. 즉, 유연성이 없는 모델의 경우 높으며 유연성이 증가하면 최소값에 도달할 때까지 감소한다. 그런 다음 분산이 증가하기 시작하고 테스트 오류가 증가하기 시작한다.
 
Bayes error.
모델의 유연성에 의존하지 않으므로 상수이다.
# 7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.
```python
import numpy as np
import pandas as pd

d = {'X1': pd.Series([0,2,0,0,-1,1]),
     'X2': pd.Series([3,0,1,1,0,1]),
     'X3': pd.Series([0,0,3,2,1,1]),
     'Y': pd.Series(['Red','Red','Red','Green','Green','Red'])}

df = pd.DataFrame(d)
df.index = np.arange(1, len(df) + 1)
df
```

X1	X2	X3	Y
1	0	3	0	Red
2	2	0	0	Red
3	0	1	3	Red
4	0	1	2	Green
5	-1	0	1	Green
6	1	1	1	Red

# Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.
## (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.
```python
from math import sqrt
df['distance']=np.sqrt(df['X1']**2+df['X2']**2+df['X3']**2)
df
```

X1	X2	X3	Y	distance
1	0	3	0	Red	3.000000
2	2	0	0	Red	2.000000
3	0	1	3	Red	3.162278
4	0	1	2	Green	2.236068
5	-1	0	1	Green	1.414214
6	1	1	1	Red	1.732051

## (b) What is our prediction with K = 1? Why?
```python
dfb=df.sort_values(['distance'])
dfb
```

X1	X2	X3	Y	distance
5	-1	0	1	Green	1.414214
6	1	1	1	Red	1.732051
2	2	0	0	Red	2.000000
4	0	1	2	Green	2.236068
1	0	3	0	Red	3.000000
3	0	1	3	Red	3.162278

K=1 일때 가장 가까운 항목(거리 1.41의 5번째)의 값이므로 Green 이다.
## (c) What is our prediction with K = 3? Why?
K=3 일때 가장 가까운 항목 3개(5,6,2)의 값을 볼때 Red 일 확률이 2/3 이고 Green일 확률이 1/3이므로 최종 예측은 Red 이다.
## (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?
K 값이 크면 비선형성이 평균화되는 것처럼 결정 경계가 더 부드러워진다. 이는 KNN이 다수결 투표를 사용하기 때문에 발생하며 이는 개별 포인트에 대한 강조가 덜하다는 것을 의미한다. K 값이 크면 지점마다 거의 달라지지 않는 결정 경계를 갖게 될 것이다. 왜냐하면 이 다수결 투표의 결과는 변경되어야 하지만 대부분의 지점에서는 이것이 다수결이 되기 때문이다. 즉, 가장 가까운 이웃 중 하나가 한 클래스에서 다른 클래스로 변경되어도 대다수의 투표는 동일하게 유지된다. 대조적으로, K가 매우 작을 때 결정 경계는 국소적 비선형성을 더 잘 포착할 수 있다. 왜냐하면 이웃의 수가 너무 적기 때문에 대부분의 이웃이 지점마다 상당히 다를 수 있기 때문이다. 따라서 K의 최고 값은 작을 것으로 예상된다.
# 10. This exercise involves the Boston housing data set.
```python
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
```
## (a) To begin, load in the Boston data set, which is part of the ISLP library
```python
df = pd.read_csv('C:/Users/sonji/Desktop/과제 모음/회귀 분석/CSVs/SCV/Boston.csv')
df.head()
```
(506, 14)

Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
0	1	0.00632	18.0	2.31	0	0.538	6.575	65.2	4.0900	1	296	15.3	4.98	24.0
1	2	0.02731	0.0	7.07	0	0.469	6.421	78.9	4.9671	2	242	17.8	9.14	21.6
2	3	0.02729	0.0	7.07	0	0.469	7.185	61.1	4.9671	2	242	17.8	4.03	34.7
3	4	0.03237	0.0	2.18	0	0.458	6.998	45.8	6.0622	3	222	18.7	2.94	33.4
4	5	0.06905	0.0	2.18	0	0.458	7.147	54.2	6.0622	3	222	18.7	5.33	36.2

## (b) How many rows are in this data set? How many columns? What do the rows and columns represent?
```python
np.shape(df)
```
(506, 14)

## (c) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your fndings.
```python
g = sns.PairGrid(df)
g.map_upper(plt.scatter, s=3)
g.map_diag(plt.hist)
g.map_lower(plt.scatter, s=3)
g.fig.set_size_inches(12, 12)
```
![image](https://github.com/user-attachments/assets/8973ea76-3ea7-4fd3-8127-34f14328b25d)
```python
plt.scatter(df['rm'], df['medv'])
plt.xlabel('rm')
plt.ylabel('medv');
```
![image](https://github.com/user-attachments/assets/f9b07cf4-9daa-40f4-acdd-dbbbc1f9e192)
rm과 대상 사이에 긍정적인 선형 관계가 존재하는 것으로 보인다. 이는 rm이 객실 수(더 많은 공간, 더 높은 가격)이기 때문에 예상된다
```python
plt.scatter(df['lstat'], df['medv'])
plt.xlabel('rm')
plt.ylabel('medv');
```
![image](https://github.com/user-attachments/assets/24e89114-063e-49f6-bc33-7dac91270f6f)
lstat와 대상은 음의 비선형 관계를 갖는 것으로 보인다. 이는 lstat가 낮은 지위의 사람들(낮은 지위, 낮은 소득, 저렴한 주택)의 비율이기 때문에 예상된다.
```python
plt.scatter(df['rm'], df['lstat'])
plt.xlabel('rm')
plt.ylabel('lstat');
```
![image](https://github.com/user-attachments/assets/7be657bf-4b29-4f59-9527-062d9c0ec3fc)
lstat와 rm 사이에는 음의 비선형 관계가 존재하는 것으로 보인다. 돈이 적은(높은 lstat) 사람들은 더 큰 주택(높은 rm)을 감당할 수 없기 때문이다.
## (d) Are any of the predictors associated with per capita crime rate? If so, explain the relationship
```python
df.corrwith(df['crim']).sort_values()
```
medv         -0.388305
dis          -0.379670
rm           -0.219247
zn           -0.200469
chas         -0.055892
ptratio       0.289946
age           0.352734
indus         0.406583
Unnamed: 0    0.407407
nox           0.420972
lstat         0.455621
tax           0.582764
rad           0.625505
crim          1.000000
dtype: float64
crim 과 가장 상관관계가 큰 3개는 rad,tax,lstat 이다.
```python
ax = sns.boxplot(x="rad", y="crim", data=df)
```
![image](https://github.com/user-attachments/assets/8758bcaa-4912-4560-b55f-90cba46637a4)
RAD가 24(가장 높은 값)이면 평균 CRIM이 훨씬 더 높고 CRIM 범위도 훨씬 더 크다.
```python
plt.scatter(df['tax'], df['crim'])
plt.xlabel('tax')
plt.ylabel('crim');
```
![image](https://github.com/user-attachments/assets/bddb6944-26c5-415d-9801-b611e9c05f8f)
TAX가 666 이면 평균 CRIM이 훨씬 더 높고 CRIM 범위도 훨씬 더 크다.
```python
plt.scatter(df['lstat'], df['crim'])
plt.xlabel('lstat')
plt.ylabel('crim');
```
![image](https://github.com/user-attachments/assets/5141bde3-2fbb-46b2-af14-207095ac6fe2)
LSTAT 값이 더 낮을 경우(< 10) CRIM은 항상 10 미만이다. LSTAT > 10일 경우 CRIM의 범위가 더 넓다. LSTAT < 20의 경우 데이터 포인트의 상당 부분이 CRIM = 0에 매우 가깝다.
## (e) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
```python
df.loc[df['crim'].nlargest(5).index]
```
	Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
380	381	88.9762	0.0	18.1	0	0.671	6.968	91.9	1.4165	24	666	20.2	17.21	10.4
418	419	73.5341	0.0	18.1	0	0.679	5.957	100.0	1.8026	24	666	20.2	20.62	8.8
405	406	67.9208	0.0	18.1	0	0.693	5.683	100.0	1.4254	24	666	20.2	22.98	5.0
410	411	51.1358	0.0	18.1	0	0.597	5.757	100.0	1.4130	24	666	20.2	10.11	15.0
414	415	45.7461	0.0	18.1	0	0.693	4.519	100.0	1.6582	24	666	20.2	36.98	7.0
```python
df.loc[df['tax'].nlargest(5).index]
```

Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
488	489	0.15086	0.0	27.74	0	0.609	5.454	92.7	1.8209	4	711	20.1	18.06	15.2
489	490	0.18337	0.0	27.74	0	0.609	5.414	98.3	1.7554	4	711	20.1	23.97	7.0
490	491	0.20746	0.0	27.74	0	0.609	5.093	98.0	1.8226	4	711	20.1	29.68	8.1
491	492	0.10574	0.0	27.74	0	0.609	5.983	98.8	1.8681	4	711	20.1	18.07	13.6
492	493	0.11132	0.0	27.74	0	0.609	5.983	83.5	2.1099	4	711	20.1	13.35	20.1
```python
df.loc[df['ptratio'].nlargest(5).index]]
```

Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
354	355	0.04301	80.0	1.91	0	0.413	5.663	21.9	10.5857	4	334	22.0	8.05	18.2
355	356	0.10659	80.0	1.91	0	0.413	5.936	19.5	10.5857	4	334	22.0	5.57	20.6
127	128	0.25915	0.0	21.89	0	0.624	5.693	96.0	1.7883	4	437	21.2	17.19	16.2
128	129	0.32543	0.0	21.89	0	0.624	6.431	98.8	1.8125	4	437	21.2	15.39	18.0
129	130	0.88125	0.0	21.89	0	0.624	5.637	94.7	1.9799	4	437	21.2	18.34	14.3
```python
df.describe()
```
	Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
count	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000
mean	253.500000	3.613524	11.363636	11.136779	0.069170	0.554695	6.284634	68.574901	3.795043	9.549407	408.237154	18.455534	12.653063	22.532806
std	146.213884	8.601545	23.322453	6.860353	0.253994	0.115878	0.702617	28.148861	2.105710	8.707259	168.537116	2.164946	7.141062	9.197104
min	1.000000	0.006320	0.000000	0.460000	0.000000	0.385000	3.561000	2.900000	1.129600	1.000000	187.000000	12.600000	1.730000	5.000000
25%	127.250000	0.082045	0.000000	5.190000	0.000000	0.449000	5.885500	45.025000	2.100175	4.000000	279.000000	17.400000	6.950000	17.025000
50%	253.500000	0.256510	0.000000	9.690000	0.000000	0.538000	6.208500	77.500000	3.207450	5.000000	330.000000	19.050000	11.360000	21.200000
75%	379.750000	3.677083	12.500000	18.100000	0.000000	0.624000	6.623500	94.075000	5.188425	24.000000	666.000000	20.200000	16.955000	25.000000
max	506.000000	88.976200	100.000000	27.740000	1.000000	0.871000	8.780000	100.000000	12.126500	24.000000	711.000000	22.000000	37.970000	50.000000
crim 표에 표시된 5개 도시는 특히 높다. tax 표에 표시된 모든 도시는 최대 tax 수준을 나타낸다. * ptratio 표는 학생-교사 비율이 높지만 그다지 고르지 않은 도시를 보여준다.
## (f) How many of the suburbs in this data set bound the Charles river?
```python
df['chas'].value_counts()[1]
```
35
## (g) What is the median pupil-teacher ratio among the towns in this data set?
```python
df['ptratio'].median()
```
19.05
## (h) Which suburb of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your fndings.
```python
df['medv'].idxmin()
```
398
```python
a = df.describe()
a.loc['range'] = a.loc['max'] - a.loc['min']
a.loc[398] = df.loc[398]
a
```
	Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
count	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000
mean	253.500000	3.613524	11.363636	11.136779	0.069170	0.554695	6.284634	68.574901	3.795043	9.549407	408.237154	18.455534	12.653063	22.532806
std	146.213884	8.601545	23.322453	6.860353	0.253994	0.115878	0.702617	28.148861	2.105710	8.707259	168.537116	2.164946	7.141062	9.197104
min	1.000000	0.006320	0.000000	0.460000	0.000000	0.385000	3.561000	2.900000	1.129600	1.000000	187.000000	12.600000	1.730000	5.000000
25%	127.250000	0.082045	0.000000	5.190000	0.000000	0.449000	5.885500	45.025000	2.100175	4.000000	279.000000	17.400000	6.950000	17.025000
50%	253.500000	0.256510	0.000000	9.690000	0.000000	0.538000	6.208500	77.500000	3.207450	5.000000	330.000000	19.050000	11.360000	21.200000
75%	379.750000	3.677083	12.500000	18.100000	0.000000	0.624000	6.623500	94.075000	5.188425	24.000000	666.000000	20.200000	16.955000	25.000000
max	506.000000	88.976200	100.000000	27.740000	1.000000	0.871000	8.780000	100.000000	12.126500	24.000000	711.000000	22.000000	37.970000	50.000000
range	505.000000	88.969880	100.000000	27.280000	1.000000	0.486000	5.219000	97.100000	10.996900	23.000000	524.000000	9.400000	36.240000	45.000000
398	399.000000	38.351800	0.000000	18.100000	0.000000	0.693000	5.453000	100.000000	1.489600	24.000000	666.000000	20.200000	30.590000	5.000000
중앙값이 가장 낮은 교외는 398이다. 다른 도시와 비교하여 이 교외는 높은 CRIM, ZN이 분위수 75% 미만, 평균 INDUS보다 높으며 찰스 강과 경계를 이루지 않으며 평균 NOX 이상, RM은 분위수 25% 미만이다. 최대 AGE, 최소값에 가까운 DIS, 최대 RAD, 분위수 75%의 TAX, PTRATIO 최대값 및 분위수 75% 이상의 LSTAT.
## (i) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.
```python
len(df[df['rm']>7])
```
64
```python
len(df[df['rm']>8])
```
13
```python
df[df['rm']>8].describe()
```
	Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
count	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000	13.000000
mean	232.307692	0.718795	13.615385	7.078462	0.153846	0.539238	8.348538	71.538462	3.430192	7.461538	325.076923	16.361538	4.310000	44.200000
std	60.915768	0.901640	26.298094	5.392767	0.375534	0.092352	0.251261	24.608723	1.883955	5.332532	110.971063	2.410580	1.373566	8.092383
min	98.000000	0.020090	0.000000	2.680000	0.000000	0.416100	8.034000	8.400000	1.801000	2.000000	224.000000	13.000000	2.470000	21.900000
25%	225.000000	0.331470	0.000000	3.970000	0.000000	0.504000	8.247000	70.400000	2.288500	5.000000	264.000000	14.700000	3.320000	41.700000
50%	233.000000	0.520140	0.000000	6.200000	0.000000	0.507000	8.297000	78.300000	2.894400	7.000000	307.000000	17.400000	4.140000	48.300000
75%	258.000000	0.578340	20.000000	6.200000	0.000000	0.605000	8.398000	86.500000	3.651900	8.000000	307.000000	17.400000	5.120000	50.000000
max	365.000000	3.474280	95.000000	19.580000	1.000000	0.718000	8.780000	93.900000	8.906700	24.000000	666.000000	20.200000	7.440000	50.000000
```python
df.describe()
```

Unnamed: 0	crim	zn	indus	chas	nox	rm	age	dis	rad	tax	ptratio	lstat	medv
count	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000
mean	253.500000	3.613524	11.363636	11.136779	0.069170	0.554695	6.284634	68.574901	3.795043	9.549407	408.237154	18.455534	12.653063	22.532806
std	146.213884	8.601545	23.322453	6.860353	0.253994	0.115878	0.702617	28.148861	2.105710	8.707259	168.537116	2.164946	7.141062	9.197104
min	1.000000	0.006320	0.000000	0.460000	0.000000	0.385000	3.561000	2.900000	1.129600	1.000000	187.000000	12.600000	1.730000	5.000000
25%	127.250000	0.082045	0.000000	5.190000	0.000000	0.449000	5.885500	45.025000	2.100175	4.000000	279.000000	17.400000	6.950000	17.025000
50%	253.500000	0.256510	0.000000	9.690000	0.000000	0.538000	6.208500	77.500000	3.207450	5.000000	330.000000	19.050000	11.360000	21.200000
75%	379.750000	3.677083	12.500000	18.100000	0.000000	0.624000	6.623500	94.075000	5.188425	24.000000	666.000000	20.200000	16.955000	25.000000
max	506.000000	88.976200	100.000000	27.740000	1.000000	0.871000	8.780000	100.000000	12.126500	24.000000	711.000000	22.000000	37.970000	50.000000
CRIM이 낮고, INDUS 비율이 낮고, 인구의 낮은 지위 비율(LSTAT)이 낮다.
